## This R package implements a lasso procedure with bagging method in the process of feature selection. Permutation test is then performed to add statistic stability, which later is approximated by generalized Pareto distribution.


data_matrix = load data from file and convert to dataframe(column with features and lines of samples)

feature_matrix, Y = data_handler(data_matrix) // seperate feature columns with Y columns


** FUCNTION LASSOBAG **
function LassoBag (feature_matrix, Y, bootN=1000,imputeN=1000,imputeN.max=2000,permut.increase=100,boot.rep=TRUE,a.family=c("gaussian","binomial","poisson","multinomial","cox","mgaussian"),parallel=F) {
    /* feature_matrix is a matrix contains all features column from raw data
       Y is the supervised training results, one column
       bootN is bootstrapping times
       inputeN is the initial permutation times
       imputeN.max is the max permutation times
    */

    // Define parameters
    total_features_number = get the column numbers of feature_matrix
    permutation_box = [[0]*imputeN]*total_features_number // a matrix: each row represents a feature and each column shows the record of showed-up times of each feature during each bagging for imputeN times of permutation.
    p_times = 0 // a tracker inside permutation loop in order to mark the original results.


    // Check the feature_matrix has the same rows as Y:
    if length(one column of feature_matrix) != length(Y):
      print("incoporate length of matrix and outVarianbles, plz check your input")
      break


    // Start permutation loop:
    for _ in imputeN:
      // clear feature box
      feature_box = [0]*total_feature_number // a list of initial ZERO of length feature numbers for every permutation

      // permutate Y
      if p_times == 0:
        p_Y = Y // keep it origin
      else:
        p_Y = sample_without_replacement(Y) // make permutations on Y


      // Start the bagging:
      for boottime in bootN:
        // generate a new dataset
        new_feature_matrix, new_Y = resample(feature_matrix, p_Y) // see definition of resample below

        // implement one lasso to reduce noise:
        lasso_features = boot.once(new_feature_matrix, new_Y) // a list of feature number of the chosen features; see definition of boot.once below

        // update the feature_box
        for chosen_feature_number in lasso_features:
          feature_box[chosen_feature_number] += 1

      // append to permutation_box:
      permutation_box[p_times] = feature_box // form a permutation matrix where columns are features and lines are p_times
      p_times += 1

    // End of Permutation loop //

    // Calculating permutation distribution and perform permutation test for each feature

    // get distribution data from permutation matrix
    permutation_matrix = dataframe(permutation_box) // get a matrix
    feature_results = [0] * total_feature_number
    for feature_num in length(row_number(permutation_matrix)):
      feature_distribution_data = permutation_matrix[feature_number]

      // perform a permutation test
      p_value = GPDpermutation(feature_distribution_data) // see the definition of GPDpermutation function below
      feature_results[feature_num] = p_value

    return feature_results

}


/* OTHER HELPER FUNCTIONS */

function resample(feature_matrix, Y){
  rows_picked = [random(0, length(feature_matrix))]*length(Y) // with replacement
  new_feature_matrix = [feature_matrix[i] for i in row_picked] // generate new sample matrix
  new_Y = [Y[i] for i in row_picked]
  return new_feature_matrix, new_Y
}

function boot.once (feature_matrix, Y) {

  //perform a lasso on feature_matrix
  calling lasso origin package here

}

function GPDpermutation (feature_distribution_data) {
  GPD_k, GPD_theta = HYB(feature_distribution_data) // see definition of HYB below
  p_value = GPD_p_value_calculation(GPD_k, GPD_theta) // see definition of GPD_p_value_calculation below
  return p_value

}

function HYB(X) {
  // optimize and solve the parameter by formula in this publication:

  Chunlin Wang & Gemai Chen (2016) A new hybrid estimation method for the generalized pareto distribution, Communications in Statistics - Theory and Methods, 45:14, 4285-4294, DOI: 10.1080/03610926.2014.919399

  return GPD_k, GPD_theta
}

function GPD_p_value_calculation(GPD_k, GPD_theta) {
  // get p_value from an known distribution
  GPD package here
}
